{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71336798",
   "metadata": {},
   "source": [
    "# Training Model From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0c725",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffede0",
   "metadata": {},
   "source": [
    "Dataset was cleaned and uploaded to huggingface at `thisisfrantz/haitian-creole-english-train` for the train set and `thisisfrantz/haitian-creole-english-test` for the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd6d21",
   "metadata": {},
   "source": [
    "\"koman _\" -> \"koman ou ye\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06b19b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fpaul\\anaconda3\\envs\\nlpENV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\fpaul\\anaconda3\\envs\\nlpENV\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fpaul\\.cache\\huggingface\\hub\\datasets--thisisfrantz--haitian-creole-english-train. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 10813/10813 [00:00<00:00, 400935.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"thisisfrantz/haitian-creole-english-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7dcb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'lang1', 'lang2'],\n",
      "        num_rows: 10813\n",
      "    })\n",
      "})\n",
      "{'id': 3042, 'lang1': 'Lidè Kiben an te di, ata John Kennedy dwe cheche fason pou kontoune anbago a.', 'lang2': 'Even John F. Kennedy had to find a way around the embargo, the Cuban leader said.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8eaf2f",
   "metadata": {},
   "source": [
    "## Load Pretrained Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55e714",
   "metadata": {},
   "source": [
    "I wanted to create a custom tokenizer but don't have enough data :( ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86973bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ht-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d996202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text: Lidè Kiben an te di, ata John Kennedy dwe cheche fason pou kontoune anbago a.\n",
      "Target text: Even John F. Kennedy had to find a way around the embargo, the Cuban leader said.\n",
      "\n",
      "Source Tokens: {'input_ids': [116, 16401, 61, 14693, 32, 7, 48, 2, 8611, 424, 29389, 9732, 108, 20212, 113, 14, 300, 2556, 4424, 276, 5887, 8, 3, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Source Token IDs: [1, 1]\n",
      "\n",
      "Target Tokens: ['▁Even', '▁John', '▁F', '.', '▁Kenne', 'dy', '▁had', '▁to', '▁find', '▁a', '▁way', '▁around', '▁the', '▁emb', 'ar', 'go', ',', '▁the', '▁Cuba', 'n', '▁leader', '▁said', '.']\n",
      "Target Token IDs: [871, 424, 1316, 3, 29389, 9732, 129, 10, 504, 8, 222, 1293, 6, 30292, 4423, 5887, 2, 6, 22471, 430, 5026, 260, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fpaul\\anaconda3\\envs\\nlpENV\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example = dataset['train'][0]\n",
    "\n",
    "source_text = example['lang1']\n",
    "target_text = example['lang2']\n",
    "print(\"Source text:\", source_text)\n",
    "print(\"Target text:\", target_text)\n",
    "\n",
    "source_tokens = tokenizer(source_text)\n",
    "source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "\n",
    "# Tokenize target (as target tokenizer)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    target_tokens = tokenizer.tokenize(target_text)\n",
    "    target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "\n",
    "print(\"\\nSource Tokens:\", source_tokens)\n",
    "print(\"Source Token IDs:\", source_ids)\n",
    "\n",
    "print(\"\\nTarget Tokens:\", target_tokens)\n",
    "print(\"Target Token IDs:\", target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6432b3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokeinze_function at 0x00000298EBA20820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   0%|          | 0/10813 [00:00<?, ? examples/s]c:\\Users\\fpaul\\anaconda3\\envs\\nlpENV\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 10813/10813 [00:04<00:00, 2194.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the whole dataset\n",
    "def tokeinze_function(example):\n",
    "    inputs = tokenizer(example['lang1'], truncation=True, padding='max_length', max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(example['lang2'], truncation=True, padding='max_length', max_length=128)\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokeinze_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd3647",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fee7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# PyTorch Format\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(tokenized_dataset['train'], batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30836d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
